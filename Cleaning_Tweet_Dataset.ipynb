{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdZUJzsgAGZ57I6jo9vJcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srijani-coder/Learning_ML_Path/blob/main/Cleaning_Tweet_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2hCx0xdWah3j",
        "outputId": "6de7b80b-01f0-466d-c199-eac77cc5eca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/* 10000 Tweets */\\n\\n\\n/*'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Read the input JSON file\n",
        "with open('/content/sample_data/10000 tweets 1.json', 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "if '/* 3830 */' in data:\n",
        "  print(True)\n",
        "\n",
        "# Define a regular expression pattern to match /* x */ and line gaps\n",
        "pattern = r'/*\\s+\\d+\\s+\\*/\\s*\\n*'\n",
        "\n",
        "# Split the data into tweet dictionaries using the pattern as a delimiter\n",
        "tweets = re.split(pattern, data)\n",
        "\n",
        "tweets[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "7BDW5FMRxfTf",
        "outputId": "774d48e0-a63b-492b-8d94-49069df32ac8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"_id\" : ObjectId(\"abe415aa5e7133a4a61c0d8317875\"),\\n    \"id\" : \"149715690143449899009\",\\n    \"objectType\" : \"activity\",\\n    \"actor\" : {\\n        \"objectType\" : \"person\",\\n        \"id\" : \"10243188921458\",\\n        \"link\" : \"http://www.twitter.com/losebabyweight1\",\\n        \"displayName\" : \"losebabyweight\",\\n        \"postedTime\" : \"2010-09-09T22:40:10.000Z\",\\n        \"image\" : \"https://pbs.twimg.com/profile_images/522696965503455233/WfF2aJ7N_normal.png\",\\n        \"summary\" : \"http://www.losebabyweight.com.au offers mums safe and proven plans to lose weight. Lose an average of 1kg a week.\",\\n        \"links\" : [ \\n            {\\n                \"href\" : \"http://www.losebabyweight.com.au\",\\n                \"rel\" : \"me\"\\n            }\\n        ],\\n        \"friendsCount\" : 218,\\n        \"followersCount\" : 1960,\\n        \"listedCount\" : 17,\\n        \"statusesCount\" : 14439,\\n        \"twitterTimeZone\" : \"Sydney\",\\n        \"verified\" : false,\\n        \"utcOffset\" : \"39600\",\\n        \"preferredUsername\" : \"losebabyweight1\",\\n        \"languages\" : [ \\n            \"en\"\\n        ],\\n        \"location\" : {\\n            \"objectType\" : \"place\",\\n            \"displayName\" : \"Australia\"\\n        },\\n        \"favoritesCount\" : 0\\n    },\\n    \"verb\" : \"post\",\\n    \"postedTime\" : \"2016-04-01T00:00:01.000Z\",\\n    \"generator\" : {\\n        \"displayName\" : \"Facebook\",\\n        \"link\" : \"http://www.facebook.com/twitter\"\\n    },\\n    \"provider\" : {\\n        \"objectType\" : \"service\",\\n        \"displayName\" : \"Twitter\",\\n        \"link\" : \"http://www.twitter.com\"\\n    },\\n    \"link\" : \"http://twitter.com/losebabyweight1/statuses/715690143449899009\",\\n    \"text\" : \"CONGRATULATIONS Suzie Walker on both your beautiful little man and your FANTASTIC commitment and hard work. You... https://t.co/m4QLVq0BTr\",\\n    \"object\" : {\\n        \"objectType\" : \"note\",\\n        \"id\" : \"1345715690143449899009\",\\n        \"summary\" : \"CONGRATULATIONS Suzie Walker on both your beautiful little man and your FANTASTIC commitment and hard work. You... https://t.co/m4QLVq0BTr\",\\n        \"link\" : \"http://twitter.com/losebabyweight1/statuses/715690143449899009\",\\n        \"postedTime\" : \"2016-04-01T00:00:01.000Z\"\\n    },\\n    \"favoritesCount\" : 0,\\n    \"twitter_entities\" : {\\n        \"hashtags\" : [],\\n        \"urls\" : [ \\n            {\\n                \"url\" : \"https://t.co/m4QLVq0BTr\",\\n                \"expanded_url\" : \"http://fb.me/WUTD9TnQ\",\\n                \"display_url\" : \"fb.me/WUTD9TnQ\",\\n                \"indices\" : [ \\n                    115, \\n                    138\\n                ]\\n            }\\n        ],\\n        \"user_mentions\" : [],\\n        \"symbols\" : []\\n    },\\n    \"twitter_filter_level\" : \"low\",\\n    \"twitter_lang\" : \"en\",\\n    \"retweetCount\" : 0,\\n    \"gnip\" : {\\n        \"matching_rules\" : [ \\n            {\\n                \"value\" : \"bio_location: \\\\\"Australia\\\\\"\",\\n                \"tag\" : null\\n            }\\n        ],\\n        \"urls\" : [ \\n            {\\n                \"url\" : \"https://t.co/m4QLVq0BTr\",\\n                \"expanded_url\" : \"https://www.facebook.com/photo.php?fbid=1257229527624685\",\\n                \"expanded_status\" : 403\\n            }\\n        ],\\n        \"klout_score\" : 44,\\n        \"language\" : {\\n            \"value\" : \"en\"\\n        },\\n        \"profileLocations\" : [ \\n            {\\n                \"objectType\" : \"place\",\\n                \"geo\" : {\\n                    \"type\" : \"point\",\\n                    \"coordinates\" : [ \\n                        135.0, \\n                        -25.0\\n                    ]\\n                },\\n                \"address\" : {\\n                    \"country\" : \"Australia\",\\n                    \"countryCode\" : \"AU\"\\n                },\\n                \"displayName\" : \"Australia\"\\n            }\\n        ]\\n    }\\n}\\n\\n/*'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAX04AIfdxER",
        "outputId": "023f9cd4-2fda-4546-af76-c243ea536592"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_cleaned_tweet(input_string):\n",
        "    # Define regular expressions for extracting relevant data\n",
        "    id_str_pattern = r'\"id\" : \"([^\"]+)\"'\n",
        "    text_pattern = r'\"text\" : \"([^\"]+)\"'\n",
        "    actor_id_pattern = r'\"actor\" : \\{[^}]*\"id\" : \"([^\"]+)\"'\n",
        "    display_name_pattern = r'\"displayName\" : \"([^\"]+)\"'\n",
        "    preferred_username_pattern = r'\"preferredUsername\" : \"([^\"]+)\"'\n",
        "    location_pattern = r'\"location\" : \\{[^}]*\"displayName\" : \"([^\"]+)\"'\n",
        "    link_pattern = r'\"link\" : \"([^\"]+)\"'\n",
        "    summary_pattern = r'\"summary\" : \"([^\"]+)\"'\n",
        "    posted_time_pattern = r'\"postedTime\" : \"([^\"]+)\"'\n",
        "    expanded_url_pattern = r'\"expanded_url\" : \"([^\"]+)\"'\n",
        "\n",
        "    # Extract data using regular expressions\n",
        "    id_str_match = re.search(id_str_pattern, input_string)\n",
        "    text_match = re.search(text_pattern, input_string)\n",
        "    actor_id_match = re.search(actor_id_pattern, input_string)\n",
        "    display_name_match = re.search(display_name_pattern, input_string)\n",
        "    preferred_username_match = re.search(preferred_username_pattern, input_string)\n",
        "    location_match = re.search(location_pattern, input_string)\n",
        "    if location_match is None:\n",
        "      location_match = '[]'\n",
        "    else:\n",
        "      location_match = location_match.group(1)\n",
        "    link_match = re.search(link_pattern, input_string)\n",
        "    summary_match = re.search(summary_pattern, input_string)\n",
        "    posted_time_match = re.search(posted_time_pattern, input_string)\n",
        "    expanded_url_match = re.search(expanded_url_pattern, input_string)\n",
        "\n",
        "    # Create the cleaned tweet dictionary\n",
        "    cleaned_tweet = {\n",
        "        \"created_at\": datetime.strptime(posted_time_match.group(1), \"%Y-%m-%dT%H:%M:%S.000Z\").strftime(\"%a %b %d %H:%M:%S +0000 %Y\"),\n",
        "        \"id_str\": id_str_match.group(1),\n",
        "        \"text\": text_match.group(1),\n",
        "        \"user\": {\n",
        "            \"id\": int(actor_id_match.group(1)),\n",
        "            \"name\": display_name_match.group(1),\n",
        "            \"screen_name\": preferred_username_match.group(1),\n",
        "            \"location\": location_match,\n",
        "            \"url\": link_match.group(1),\n",
        "            \"description\": summary_match.group(1)\n",
        "        },\n",
        "        \"place\": {},\n",
        "        \"entities\": {\n",
        "            \"hashtags\": [],\n",
        "            \"urls\": [],\n",
        "            \"user_mentions\": []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Extract hashtags and URLs\n",
        "    # Generate a regex pattern to extract hashtags\n",
        "    hashtags_pattern = r'\"twitter_entities\"\\s*:\\s*\\{(?:\\s*\"hashtags\"\\s*:\\s*\\[\\s*\\{[^}]*\"text\"\\s*:\\s*\"([^\"]+)\"[^}]*\\]\\s*)?\\}'\n",
        "    # Find hashtags using the pattern\n",
        "    hashtags_match = re.search(hashtags_pattern, input_string)\n",
        "\n",
        "    # Initialize a list to store extracted hashtags\n",
        "    extracted_hashtags = []\n",
        "\n",
        "    # Check if \"twitter_entities\" and \"hashtags\" exist and are not empty\n",
        "    if hashtags_match and hashtags_match.group(1):\n",
        "      extracted_hashtags = hashtags_match.group(1).split('\",\"')\n",
        "\n",
        "    urls = re.findall(expanded_url_pattern, input_string)\n",
        "\n",
        "    for hashtag in extracted_hashtags:\n",
        "        cleaned_tweet[\"entities\"][\"hashtags\"].append(hashtag)\n",
        "\n",
        "    for url in urls:\n",
        "        cleaned_tweet[\"entities\"][\"urls\"].append({\n",
        "            \"url\": url,\n",
        "            \"unwound\": {\n",
        "                \"url\": url,\n",
        "                \"title\": \"\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "# Example usage:\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "OxN-7xcmxoEu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pz6kp6l6Msfs"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUiPP-glXRQ_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweet_list = []\n",
        "error_indices = []  # To store the indices of items that raise exceptions\n",
        "i = 1\n",
        "\n",
        "while i <= 10000:\n",
        "    try:\n",
        "        cl_twt = extract_cleaned_tweet(tweets[i])\n",
        "        cleaned_tweet_list.append(cl_twt)\n",
        "    except Exception as e:\n",
        "        error_indices.append(i)  # Add the index to the error_indices list\n",
        "        print(f\"Error at index {i}: {str(e)}\")\n",
        "    finally:\n",
        "        i = i + 1\n",
        "\n",
        "# Print the indices of items that raised exceptions\n",
        "print(\"Indices of items with errors:\", error_indices)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW99pDjkc8VK",
        "outputId": "59d13c5f-df75-4d90-d6f7-0a45d45072c4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indices of items with errors: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(error_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ev1eT9adsUk",
        "outputId": "ddd75869-3678-45b3-cdd2-73e7b3802653"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_tweet_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aseQAhfClgbo",
        "outputId": "90200e46-462e-4486-a3ed-f4c1c5dfa18a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cleaned_tweet.json', 'w+') as file:\n",
        "  file.write('[')\n",
        "  i = 0\n",
        "  while(i < 10000):\n",
        "    file.write(str(cleaned_tweet_list[i]) + \",\\n\")\n",
        "    i = i + 1\n",
        "  file.write(']')"
      ],
      "metadata": {
        "id": "jPr0K6I6loQ6"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}